import os
import re
import glob
import math
import gc
import argparse
import random
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

import numpy as np
import pandas as pd

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)

from datasets import Dataset
from peft import LoraConfig, TaskType, get_peft_model, PeftModel
import matplotlib.pyplot as plt

# Utility

def set_seed(seed: int = 42) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def norm_text(s: str) -> str:
    s = str(s)
    s = re.sub(r"\s+", " ", s.strip())
    return s


def get_dtype() -> torch.dtype:
    # bfloat16 se disponibile (A100/H100), altrimenti float16 o float32 su CUDA
    if torch.cuda.is_available():
        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    return torch.float32


def find_adapter_dir(base_dir: str) -> str:
    if os.path.exists(os.path.join(base_dir, "adapter_config.json")):
        return base_dir
    cands = sorted(glob.glob(os.path.join(base_dir, "checkpoint-*")), reverse=True)
    for d in cands:
        if os.path.exists(os.path.join(d, "adapter_config.json")):
            return d
    raise FileNotFoundError(f"adapter_config.json non trovato in {base_dir} nÃ© nei checkpoint-*")

# Data collator (labels pad = -100)

@dataclass
class DataCollatorForCausalLMWithPadding:
    tokenizer: Any
    label_pad_token_id: int = -100

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        batch = self.tokenizer.pad(features, padding=True, return_tensors="pt")
        batch["labels"] = batch["labels"].clone()
        batch["labels"][batch["attention_mask"] == 0] = self.label_pad_token_id
        return batch

# Training (LoRA SFT)

def build_tokenizer(model_id: str, padding_side: str = "right") -> AutoTokenizer:
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    tok.padding_side = padding_side
    return tok


def load_base_model(model_id: str) -> AutoModelForCausalLM:
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=get_dtype(),
    )
    model.config.use_cache = False
    model.gradient_checkpointing_enable()
    return model


def apply_lora(model: AutoModelForCausalLM,
               r: int = 8,
               alpha: int = 32,
               dropout: float = 0.05,
               target_modules: Optional[List[str]] = None) -> AutoModelForCausalLM:
    if target_modules is None:
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"] #per Minerva
        target_modules=["query_proj", "key_proj", "value_proj", "dense"] #per Italia

    cfg = LoraConfig(
        r=r,
        lora_alpha=alpha,
        lora_dropout=dropout,
        bias="none",
        target_modules=target_modules,
        task_type=TaskType.CAUSAL_LM,
    )
    model = get_peft_model(model, cfg)
    model.print_trainable_parameters()
    return model


def build_assistant_only_dataset(
    df: pd.DataFrame,
    tokenizer: AutoTokenizer,
    max_len: int = 512,
    seed: int = 42,
) -> Tuple[Dataset, Dataset]:

    df = df[["system", "user", "assistant"]].dropna().astype(str).reset_index(drop=True)
    raw_ds = Dataset.from_pandas(df)

    split = raw_ds.train_test_split(test_size=0.1, seed=seed) #solo su Italia
    train_ds = split["train"]
    eval_ds  = split["test"]

# TOKENIZZAZIONE

 def tokenize_minerva(ex):    #per Minerva
    sys = ex["system"].strip()
    usr = ex["user"].strip()
    ans = ex["assistant"].strip()

    messages_prompt = [
        {"role": "system", "content": sys},
        {"role": "user", "content": usr},
    ]
    messages_full = messages_prompt + [{"role": "assistant", "content": ans}]

    prompt_ids = tokenizer.apply_chat_template(
        messages_prompt,
        tokenize=True,
        add_generation_prompt=True,
        truncation=True,
        max_length=max_len,
        padding=False,
    )

    full_ids = tokenizer.apply_chat_template(
        messages_full,
        tokenize=True,
        add_generation_prompt=False,
        truncation=True,
        max_length=max_len,
        padding=False,
    )

    prompt_len = min(len(prompt_ids), len(full_ids))
    labels = [-100] * len(full_ids)
    if prompt_len < len(full_ids):
        labels[prompt_len:] = full_ids[prompt_len:]

    return {
        "input_ids": full_ids,
        "attention_mask": [1] * len(full_ids),
        "labels": labels,
    }


def tokenize_italia(ex):      # per Italia
    sys = ex["system"].strip()
    usr = ex["user"].strip()
    ans = ex["assistant"].strip()

    prompt_text = (
        "<|system|>\n" + sys + "</s>\n"
        "<|user|>\n" + usr + "</s>\n"
        "<|assistant|>\n"
    )
    full_text = prompt_text + ans

    prompt_ids = tokenizer(
        prompt_text,
        truncation=True,
        max_length=max_len,
        padding=False,
        add_special_tokens=False,
    )["input_ids"]

    full = tokenizer(
        full_text,
        truncation=True,
        max_length=max_len,
        padding=False,
        add_special_tokens=False,
    )
    full_ids = full["input_ids"]
    attn = full["attention_mask"]

    prompt_len = min(len(prompt_ids), len(full_ids))
    labels = [-100] * len(full_ids)
    if prompt_len < len(full_ids):
        labels[prompt_len:] = full_ids[prompt_len:]

    return {
        "input_ids": full_ids,
        "attention_mask": attn,
        "labels": labels,
    }

    # Optim fused se disponibile, altrimenti fallback
    optim = "adamw_torch_fused"
    try:
        _ = TrainingArguments(output_dir=out_dir, optim=optim)
    except Exception:
        optim = "adamw_torch"

    args = TrainingArguments(
        output_dir=out_dir,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=grad_accum,
        num_train_epochs=num_epochs,
        learning_rate=lr,
        logging_steps=10,

        evaluation_strategy="steps",
        eval_steps=eval_steps,
        save_strategy="steps",
        save_steps=save_steps,
        save_total_limit=2,

        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,

        remove_unused_columns=False,
        bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),
        fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),
        gradient_checkpointing=True,
        optim=optim,
        report_to="none",
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    trainer.train()

    os.makedirs(out_dir, exist_ok=True)
    trainer.save_model(out_dir)
    tokenizer.save_pretrained(out_dir)

    print(f"Adapter salvato in: {out_dir}")
